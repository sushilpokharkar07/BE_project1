{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from imutils import face_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \".\"\n",
    "checkpoint_path = os.path.join(base_dir, '../logs/model/siamese-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code cell is taken from the source code of keras_vggface.'\n",
    "I tried using the preprocess_input function provided by tf.keras but they provide different results.\n",
    "To my knowledge, it seems that the mean values which are subtracted in each image are different.\n",
    "'''\n",
    "K = tf.keras.backend\n",
    "\n",
    "def preprocess_input(x, data_format=None, version=1):\n",
    "    x_temp = np.copy(x)\n",
    "    if data_format is None:\n",
    "        data_format = K.image_data_format()\n",
    "    assert data_format in {'channels_last', 'channels_first'}\n",
    "\n",
    "    if version == 1:\n",
    "        if data_format == 'channels_first':\n",
    "            x_temp = x_temp[:, ::-1, ...]\n",
    "            x_temp[:, 0, :, :] -= 93.5940\n",
    "            x_temp[:, 1, :, :] -= 104.7624\n",
    "            x_temp[:, 2, :, :] -= 129.1863\n",
    "        else:\n",
    "            x_temp = x_temp[..., ::-1]\n",
    "            x_temp[..., 0] -= 93.5940\n",
    "            x_temp[..., 1] -= 104.7624\n",
    "            x_temp[..., 2] -= 129.1863\n",
    "\n",
    "    elif version == 2:\n",
    "        if data_format == 'channels_first':\n",
    "            x_temp = x_temp[:, ::-1, ...]\n",
    "            x_temp[:, 0, :, :] -= 91.4953\n",
    "            x_temp[:, 1, :, :] -= 103.8827\n",
    "            x_temp[:, 2, :, :] -= 131.0912\n",
    "        else:\n",
    "            x_temp = x_temp[..., ::-1]\n",
    "            x_temp[..., 0] -= 91.4953\n",
    "            x_temp[..., 1] -= 103.8827\n",
    "            x_temp[..., 2] -= 131.0912\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return x_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggface = tf.keras.models.Sequential()\n",
    "vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\", input_shape=(224,224, 3)))\n",
    "vggface.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.Convolution2D(512, (3, 3), activation='relu', padding=\"SAME\"))\n",
    "vggface.add(tf.keras.layers.MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "vggface.add(tf.keras.layers.Flatten())\n",
    "\n",
    "vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "vggface.add(tf.keras.layers.Dropout(0.5))\n",
    "vggface.add(tf.keras.layers.Dense(4096, activation='relu'))\n",
    "vggface.add(tf.keras.layers.Dropout(0.5))\n",
    "vggface.add(tf.keras.layers.Dense(2622, activation='softmax'))\n",
    "\n",
    "vggface.pop()\n",
    "vggface.add(tf.keras.layers.Dense(128, use_bias=False))\n",
    "\n",
    "for layer in vggface.layers[:-2]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(tf.keras.Model):\n",
    "    def __init__(self, vgg_face):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.vgg_face = vgg_face\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        image_1, image_2, image_3 =  inputs\n",
    "        with tf.name_scope(\"Anchor\") as scope:\n",
    "            feature_1 = self.vgg_face(image_1)\n",
    "            feature_1 = tf.math.l2_normalize(feature_1, axis=-1)\n",
    "        with tf.name_scope(\"Positive\") as scope:\n",
    "            feature_2 = self.vgg_face(image_2)\n",
    "            feature_2 = tf.math.l2_normalize(feature_2, axis=-1)\n",
    "        with tf.name_scope(\"Negative\") as scope:\n",
    "            feature_3 = self.vgg_face(image_3)\n",
    "            feature_3 = tf.math.l2_normalize(feature_3, axis=-1)\n",
    "        return [feature_1, feature_2, feature_3]\n",
    "    \n",
    "    @tf.function\n",
    "    def get_features(self, inputs):\n",
    "        return tf.math.l2_normalize(self.vgg_face(inputs), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork(vggface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model([tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3)), tf.zeros((32,224,224,3))])\n",
    "_ = model.get_features(tf.zeros((32,224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Details\n",
    "The cell below should be re-run for data collection for multiple people.\n",
    "For a single person, you could  collect 8-10 images tops. Entering the name of the person would add a directory to the data folder which will be the name of that person. \n",
    "If the name of the person exists it will give an error. So make sure to keep different names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Enter the name of the person : \")\n",
    "os.mkdir(os.path.join(data_dir, name))\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Image', frame)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k == ord('s'):\n",
    "        cv2.imwrite(os.path.join(data_dir, name + '/' + str(count) + '.png') , frame)\n",
    "        count += 1\n",
    "    if k ==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = sorted(os.listdir(data_dir))\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "features = []\n",
    "dumpable_features = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in people:\n",
    "    person_path = os.path.join(data_dir, person)\n",
    "    print(person_path)\n",
    "    images = []\n",
    "    for image in os.listdir(person_path):\n",
    "        image_path = os.path.join(person_path, image)\n",
    "        img = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_detector(gray, 0)\n",
    "        if len(faces) == 1:\n",
    "            for face in faces:\n",
    "                face_bounding_box = face_utils.rect_to_bb(face)\n",
    "                if all(i >= 0 for i in face_bounding_box):\n",
    "                    [x, y, w, h] = face_bounding_box\n",
    "                    frame = img[y:y + h, x:x + w]\n",
    "                    frame = cv2.resize(frame, (224, 224))\n",
    "                    frame = np.asarray(frame, dtype=np.float64)\n",
    "                    images.append(frame)\n",
    "    images = np.asarray(images)\n",
    "    images = preprocess_input(images)\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    feature = model.get_features(images)\n",
    "    feature = tf.reduce_mean(feature, axis=0)\n",
    "    features.append(feature.numpy())\n",
    "    dumpable_features[person] = feature.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping Features Rather than Calculating Them every time\n",
    "\n",
    "I have added a variable dumpable_features which could be pickled. It will save a dictionary format of {'name':'features (numpy array)'} for each person. Simply pickle using the following commands\n",
    "```\n",
    "with open('weigths.pkl', 'wb') as f:\n",
    "    pickle.dump(dumpable_features, f) \n",
    "```\n",
    "and reload them using\n",
    "```\n",
    "with open('weigths.pkl', 'rb') as f:\n",
    "    dumpable_features_reloaded = pickle.load(f)\n",
    "    \n",
    "people = []\n",
    "features = []\n",
    "for key, value in dumpable_features_reloaded.items():\n",
    "    people.append(key)\n",
    "    features.append(value)\n",
    "```\n",
    "During my testing I tested on 8-10 people so regenerating features wasn't that much slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.asarray(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "name = 'not identified'\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = face_detector(gray, 0)\n",
    "    for face in faces:\n",
    "        face_bounding_box = face_utils.rect_to_bb(face)\n",
    "        if all(i >= 0 for i in face_bounding_box):\n",
    "            [x, y, w, h] = face_bounding_box\n",
    "            frame = img[y:y + h, x:x + w]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "            frame = np.asarray(frame, dtype=np.float64)\n",
    "            frame = np.expand_dims(frame, axis=0)\n",
    "            frame = preprocess_input(frame)\n",
    "            feature = model.get_features(frame)\n",
    "                \n",
    "            dist = tf.norm(features - feature, axis=1)\n",
    "            name = 'not identified'\n",
    "            loc = tf.argmin(dist)\n",
    "            if dist[loc] < 0.8:\n",
    "                name = people[loc]\n",
    "            else:\n",
    "#                     print(dist.numpy())\n",
    "                pass\n",
    "                    \n",
    "            font_face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(img, name, (x, y-5), font_face, 0.8, (0,0,255), 3)\n",
    "    cv2.imshow('Image', img)\n",
    "    k = cv2.waitKey(1)\n",
    "    if k ==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
